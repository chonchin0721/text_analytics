---
title: "Delta Customer Service Tweets Processing"
author: "Jessica She"
date: '2023-05-20'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text Mining approach 

The Bag of Words approach is a text mining technique used on text data that unavoidably lacks a solid syntactic foundation. Social media data is one such instance of this kind of text data. On the other hand, text data with pure syntactic structure, as that found in legal texts, can be analyzed using syntactic parsing. Some of the bag-of-words-related analysis in this markdown file is done with the help of R string-specific libraries like "stringr," "tm," and base functions.

To work on the stringr functions, refer to the stringr cheatsheet uploaded to the moodle page. 


```{r Load the required libraries}
library(tidyr)
library(dplyr)
library(stringr)
library(readr)
library(tm)
library(qdap)
```

## Including Plots

```{r Read Delta Airlines Tweets File - Use read_csv}
library(readr)
tweets_data <- read_csv(str_c(getwd(),"/Input/HW1 - Input_Tweets.csv"))
tweets_data

```

```{r capture the tweet length into tweet_length column. Use %>% piping. Use mutate to create new columns}
tweets_data <- tweets_data %>%
  select(weekday, month, date, year, text) %>%
  mutate(tweets_length = str_count(tweets_data$text))
tweets_data

# Capturing the average length
avg_length <- mean(tweets_data$tweets_length)
avg_length
```


```{r removing short tweets}
# What percentage of short tweets (tweets less than 50 characters in length) are there in the dataset? Use nchar to answer this question. 
sum(nchar(tweets_data$text) < 50)/nrow(tweets_data) * 100

# Create a new dataset tweets_data_subset without short tweets. Use subset() function from base R installation. 
tweets_data2 <- subset(tweets_data, nchar(as.character(text)) >= 50)
tweets_data2
```

```{r convert tweets to lowercase}
tweets_data$text <- str_to_lower(tweets_data$text)
```


```{r tweets with "pls" word}
str_detect(tweets_data$text, "pls") %>% sum()
```

```{r replace pls with please using sub, include=FALSE}
sub("pls", "please", tweets_data$text)
```


```{r Replace "pls" and "thanks" with "please" and "thank you" respectively in a single statement using mgsub, include=FALSE}
mgsub(c("pls", "thx"), c("please", "thanks"), tweets_data$text)

```


```{r extract_response_date_from_month_day_year} 
library(qdap)
library(lubridate)

patterns <-c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')
replacements <-seq(1:12)
tweets_data$month_numeric <- mgsub(patterns,replacements,tweets_data$month)

patterns_weekday <- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')
replacements_weekday <- seq(1:7)
tweets_data$weekday_numeric <- mgsub(patterns_weekday,replacements_weekday,tweets_data$weekday)

tweets_data$response_date <- str_c(tweets_data$month_numeric, tweets_data$date, tweets_data$year, sep = "/")
tweets_data$response_date <- as.Date(tweets_data$response_date, format = "%m/%d/%Y")
tweets_data
ymd(as.Date("2023-05-19"))

```

```{r extract_the agent_code}
tweets_data$agent <- str_replace_na(str_remove(str_extract(tweets_data$text, "\\*[:alnum:]+$"), "\\*"), replacement = "")

```

```{r extract_the_most_hard_working_agent}
agent_extracted <- tweets_data$agent
agent_extracted[agent_extracted == ""] <- NA
names(sort(table(agent_extracted),decreasing=TRUE)[1])
```

```{r Using group_by function to capture the mean number of responses per agent and by date}

# Calculate the number of responses per employee per day
response_counts <- tweets_data %>%
  filter(agent !="") %>%
  group_by(response_date, agent) %>%
  summarise(responses = n())
response_counts

sd(response_counts$responses)
mean(response_counts$responses)

#Calculate the average number of responses per day per employee
avg_responses <- response_counts %>%
  group_by(agent) %>%
  summarise(average_per_day = mean(responses))
avg_responses


```

```{r Count the total number of agents handling the cases by day}
# Counted Unique
total_number_of_agents <- tweets_data %>%
  group_by(response_date) %>%
  summarise(total_agents = n_distinct(agent))

total_number_of_agents
```

What is the typical word length for a social customer service response?
Answer: The typical word length for a social customer service response is 92.143

Which links were mentioned the most? (Not Required For the Assignment)


 Which customer representation (agent) is the most hard-working (addresses maximum responses)?
Answer: agent 'pl' is the most hardworking. There are 95 responses from him.
```{r}
sort(table(tweets_data$agent),decreasing=TRUE)[1:3]
```


How many social media responses can a customer care agent reasonably handle?
Answer: the maximum social media responses is 100, therefore a customer care agent can reasonably handle 100 social media responses. 